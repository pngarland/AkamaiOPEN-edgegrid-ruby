{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fe523821",
      "metadata": {
        "id": "fe523821"
      },
      "source": [
        "\n",
        "# Project‚ÄØ1: Build an LLM Playground\n",
        "\n",
        "Welcome! In this project, you‚Äôll learn foundations of large language models (LLMs). We‚Äôll keep the code minimal and the explanations high‚Äëlevel so that anyone who can run a Python cell can follow along.  \n",
        "\n",
        "We'll be using Google Colab for this project. Colab is a free, browser-based platform that lets you run Python code and machine learning models without installing anything on your local computer. Click the button below to open this notebook directly in Google Colab and get started!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb8584e",
      "metadata": {
        "id": "fdb8584e"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bytebyteai/ai-eng-projects/blob/main/project_1/lm_playground.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e82492",
      "metadata": {
        "id": "08e82492"
      },
      "source": [
        "---\n",
        "## Learning Objectives  \n",
        "* **Tokenization** and how raw text is tokenized into a sequene of discrete tokens\n",
        "* Inspect **GPT2** and **Transformer architecture**\n",
        "* Loading pre-trained LLMs using **Hugging Face**\n",
        "* **Decoding strategies** to generate text from LLMs\n",
        "* Completion versus **intrusction fine-tuned** LLMs\n",
        "\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1235110e",
      "metadata": {
        "id": "1235110e"
      },
      "outputs": [],
      "source": [
        "import torch, transformers, tiktoken\n",
        "print(\"torch\", torch.__version__, \"| transformers\", transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c1eb0b",
      "metadata": {
        "id": "d4c1eb0b"
      },
      "source": [
        "# 1 - Tokenization\n",
        "\n",
        "A neural network can‚Äôt digest raw text. They need **numbers**. Tokenization is the process of converting text into IDs. In this section, you'll learn how tokenization is implemented in practice.\n",
        "\n",
        "Tokenization methods generally fall into three categories:\n",
        "1. Word-level\n",
        "2. Character-level\n",
        "3. Subword-level"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d234dc0",
      "metadata": {
        "id": "1d234dc0"
      },
      "source": [
        "### 1.1 - Word‚Äëlevel tokenization\n",
        "\n",
        "Split text on whitespace and store each **word** as a token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d784a288",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d784a288",
        "outputId": "7b713e4c-9de4-4741-e00f-1ea6859f6216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 22 words\n",
            "First 15 vocab entries: ['[PAD]', '[UNK]', 'The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'Tokenization', 'converts', 'text', 'to']\n",
            "\n",
            "Input text : The brown unicorn jumps\n",
            "Token IDs  : [2, 4, 1, 6]\n",
            "Decoded    : ['The', 'brown', '[UNK]', 'jumps']\n"
          ]
        }
      ],
      "source": [
        "# 1. Tiny corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"Tokenization converts text to numbers\",\n",
        "    \"Large language models predict the next token\"\n",
        "]\n",
        "\n",
        "# 2. Build the vocabulary\n",
        "PAD, UNK = \"[PAD]\", \"[UNK]\"\n",
        "vocab = [PAD, UNK]\n",
        "word2id = {PAD: 0, UNK: 1}\n",
        "id2word = {0: PAD, 1: UNK}\n",
        "\n",
        "\n",
        "for txt in corpus:\n",
        "    for word in txt.split(\" \"):\n",
        "        if word not in word2id:\n",
        "            cur = len(vocab)\n",
        "            word2id[word] = cur\n",
        "            id2word[cur] = word\n",
        "            vocab.append(word)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)} words\")\n",
        "print(\"First 15 vocab entries:\", vocab[:15])\n",
        "\n",
        "# 3. Encode / decode\n",
        "def encode(text):\n",
        "    result = []\n",
        "    for word in text.split(\" \"):\n",
        "        if word in word2id:\n",
        "            result.append(word2id[word])\n",
        "        else:\n",
        "            result.append(word2id[UNK])\n",
        "    return result\n",
        "\n",
        "def decode(ids):\n",
        "    result = []\n",
        "    for cur in ids:\n",
        "        result.append(id2word[cur])\n",
        "    return result\n",
        "\n",
        "# 4. Demo\n",
        "sample = \"The brown unicorn jumps\"\n",
        "ids = encode(sample)\n",
        "recovered = decode(ids)\n",
        "\n",
        "print(\"\\nInput text :\", sample)\n",
        "print(\"Token IDs  :\", ids)\n",
        "print(\"Decoded    :\", recovered)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0edab2c2",
      "metadata": {
        "id": "0edab2c2"
      },
      "source": [
        "Word-level tokenization has two major limitations:\n",
        "1. Large vocabulary size\n",
        "2. Out-of-vocabulary (OOV) issue"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a379bac7",
      "metadata": {
        "id": "a379bac7"
      },
      "source": [
        "### 1.2 - Character‚Äëlevel tokenization\n",
        "\n",
        "Every single character (including spaces and emojis) gets its own ID. This guarantees zero out‚Äëof‚Äëvocabulary issues but very long sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4ac29144",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ac29144",
        "outputId": "67da7f6e-9a8c-4d0e-a420-14e92fe4aac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 54 (52 letters + 2 specials)\n",
            "\n",
            "Input text : Hello\n",
            "Token IDs  : [35, 6, 13, 13, 16]\n",
            "Decoded    : ['H', 'e', 'l', 'l', 'o']\n"
          ]
        }
      ],
      "source": [
        "# 1. Build a fixed vocabulary # a‚Äìz + A‚ÄìZ + padding + unkwown\n",
        "import string\n",
        "PAD, UNK = \"[PAD]\", \"[UNK]\"\n",
        "vocab = [PAD, UNK]\n",
        "char2id = {PAD: 0, UNK: 1}\n",
        "id2char = {0: PAD, 1: UNK}\n",
        "\n",
        "lower = [chr(i) for i in range(ord('a'), ord('z') + 1)]\n",
        "upper = [chr(i) for i in range(ord('A'), ord('Z') + 1)]\n",
        "\n",
        "for char in lower:\n",
        "    cur = len(vocab)\n",
        "    vocab.append(char)\n",
        "    char2id[char] = cur\n",
        "    id2char[cur] = char\n",
        "\n",
        "for char in upper:\n",
        "    cur = len(vocab)\n",
        "    vocab.append(char)\n",
        "    char2id[char] = cur\n",
        "    id2char[cur] = char\n",
        "\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)} (52 letters + 2 specials)\")\n",
        "\n",
        "def get_id(char):\n",
        "    return char2id.get(char) or char2id[UNK]\n",
        "\n",
        "def get_char(char):\n",
        "    return id2char.get(char) or id2char[UNK]\n",
        "\n",
        "# 2. Encode / decode\n",
        "def encode(text):\n",
        "    result = []\n",
        "    for char in text:\n",
        "        result.append(get_id(char))\n",
        "    return result\n",
        "\n",
        "def decode(ids):\n",
        "    result = []\n",
        "    for cur in ids:\n",
        "        result.append(get_char(cur))\n",
        "    return result\n",
        "\n",
        "# 3. Demo\n",
        "sample = \"Hello\"\n",
        "ids = encode(sample)\n",
        "recovered = decode(ids)\n",
        "\n",
        "print(\"\\nInput text :\", sample)\n",
        "print(\"Token IDs  :\", ids)\n",
        "print(\"Decoded    :\", recovered)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391275bd",
      "metadata": {
        "id": "391275bd"
      },
      "source": [
        "### 1.3 - Subword‚Äëlevel tokenization\n",
        "\n",
        "Sub-word methods such as `Byte-Pair Encoding (BPE)`, `WordPiece`, and `SentencePiece` **learn** the most common character and gorup them into new tokens. For example, the word `unbelievable` might turn into three tokens: `[\"un\", \"believ\", \"able\"]`. This approach strikes a balance between word-level and character-level methods and fix their limitations.\n",
        "\n",
        "For example, `BPE` algorithm forms the vocabulary using the following steps:\n",
        "1. **Start with bytes** ‚Üí every character is its own token.  \n",
        "2. **Count all adjacent pairs** in a huge corpus.  \n",
        "3. **Merge the most frequent pair** into a new token.  \n",
        "   *Repeat steps 2-3* until you hit the target vocab size (e.g., 50 k).\n",
        "\n",
        "Let's see `BPE` in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4675e67a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "4675e67a",
        "outputId": "99a58726-7976-477c-c954-b6839656fe1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-2-2b.\n401 Client Error. (Request ID: Root=1-68e4506a-183705d85d25227f4a01b93e;366cff28-2cfb-42b0-975b-5c8745c83165)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2-2b/resolve/main/config.json.\nAccess to model google/gemma-2-2b is restricted. You must have access to it and be authenticated to access it. Please log in.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/google/gemma-2-2b/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1011\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1547\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1464\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    423\u001b[0m             )\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-68e4506a-183705d85d25227f4a01b93e;366cff28-2cfb-42b0-975b-5c8745c83165)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2-2b/resolve/main/config.json.\nAccess to model google/gemma-2-2b is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4150552284.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbpe_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google/gemma-2-2b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 2. Encode / decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1076\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m   1079\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    722\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    543\u001b[0m                 \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                 \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-2-2b.\n401 Client Error. (Request ID: Root=1-68e4506a-183705d85d25227f4a01b93e;366cff28-2cfb-42b0-975b-5c8745c83165)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2-2b/resolve/main/config.json.\nAccess to model google/gemma-2-2b is restricted. You must have access to it and be authenticated to access it. Please log in."
          ]
        }
      ],
      "source": [
        "# 1. Load a pretrained BPE tokenizer (GPT-2 uses BPE).\n",
        "# Refer to  https://huggingface.co/docs/transformers/en/fast_tokenizers\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "bpe_tok = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
        "\n",
        "# 2. Encode / decode\n",
        "def encode(text):\n",
        "    return bpe_tok.encode(text)\n",
        "\n",
        "def decode(ids):\n",
        "    return bpe_tok.decode(ids)\n",
        "\n",
        "# 3. Demo\n",
        "sample = \"Unbelievable tokenization powers! üöÄ\"\n",
        "ids = encode(sample)\n",
        "recovered = decode(ids)\n",
        "\n",
        "print(\"\\nInput text :\", sample)\n",
        "print(\"Token IDs  :\", ids)\n",
        "print(\"Tokens     :\", bpe_tok.convert_ids_to_tokens(ids))\n",
        "print(\"Decoded    :\", recovered)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "badaa5a8",
      "metadata": {
        "id": "badaa5a8"
      },
      "source": [
        "### 1.4 - TikToken\n",
        "\n",
        "`tiktoken` is a production-ready library which offers high‚Äëspeed tokenization used by OpenAI models.  \n",
        "Let's compare the older **gpt2** encoding with the newer **cl100k_base** used in GPT‚Äë4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7704c470",
      "metadata": {
        "id": "7704c470"
      },
      "outputs": [],
      "source": [
        "# Use gpt2 and cl100k_base to encode and decode the following text\n",
        "# Refer to https://github.com/openai/tiktoken\n",
        "import tiktoken\n",
        "\n",
        "sentence = \"The üåü star-player scored 40 points!\"\n",
        "\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8c1023",
      "metadata": {
        "id": "5e8c1023"
      },
      "source": [
        "Experiment: try new sentences, emojis, code snippets, or other languages. If you are interested, try implementing the BPE algorithm yourself.\n",
        "\n",
        "### 1.5 - Key Takeaways\n",
        "\n",
        "* **Word‚Äëlevel**: simple but brittle (OOV problems).  \n",
        "* **Character‚Äëlevel**: robust but produces long sequences.  \n",
        "* **BPE / Byte‚ÄëLevel BPE**: middle ground used by most LLMs.  \n",
        "* **tiktoken**: shows how production models tokenize with pre‚Äëtrained sub‚Äëword vocabularies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a758ba",
      "metadata": {
        "id": "c2a758ba"
      },
      "source": [
        "# 2. What is a Language Model?\n",
        "\n",
        "At its core, a **language model (LM)** is just a *very large* mathematical function built from many neural-network layers.  \n",
        "Given a sequence of tokens `[t‚ÇÅ, t‚ÇÇ, ‚Ä¶, t‚Çô]`, it learns to output a probability for the next token `t‚Çô‚Çä‚ÇÅ`.\n",
        "\n",
        "\n",
        "Each layer applies a simple operation (matrix multiplication, attention, etc.). Stacking hundreds of these layers lets the model capture patterns and statistical relations from text. The final output is a vector of scores that says, ‚Äúhow likely is each possible token to come next?‚Äù\n",
        "\n",
        "> Think of the whole network as **one gigantic equation** whose parameters were tuned during training to minimize prediction error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f0c7399",
      "metadata": {
        "id": "8f0c7399"
      },
      "source": [
        "\n",
        "### 2.1 - A Single `Linear` Layer\n",
        "\n",
        "Before we explore Transformer, let‚Äôs start tiny:\n",
        "\n",
        "* A **Linear layer** performs `y = Wx + b`  \n",
        "  * `x` ‚Äì input vector  \n",
        "  * `W` ‚Äì weight matrix (learned)  \n",
        "  * `b` ‚Äì bias vector (learned)\n",
        "\n",
        "Although this looks basic, chaining thousands of such linear transforms (with nonlinearities in between) gives neural nets their expressive power.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e425948a",
      "metadata": {
        "id": "e425948a"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(Linear, self).__init__()\n",
        "        \"\"\"\n",
        "        YOUR CODE HERE\n",
        "        \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        YOUR CODE HERE\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e5e225",
      "metadata": {
        "id": "13e5e225"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn, torch\n",
        "\n",
        "lin = nn.Linear(3, 2)\n",
        "x = torch.tensor([1.0, -1.0, 0.5])\n",
        "print(\"Input :\", x)\n",
        "print(\"Weights:\", lin.weight)\n",
        "print(\"Bias   :\", lin.bias)\n",
        "print(\"Output :\", lin(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04f56bf",
      "metadata": {
        "id": "a04f56bf"
      },
      "source": [
        "### 2.2 - A `Transformer` Layer\n",
        "\n",
        "Most LLMs are a **stack of identical Transformer blocks**. Each block fuses two main components:\n",
        "\n",
        "| Step | What it does | Where it lives in code |\n",
        "|------|--------------|------------------------|\n",
        "| **Multi-Head Self-Attention** | Every token looks at every other token and decides *what matters*. | `block.attn` |\n",
        "| **Feed-Forward Network (MLP)** | Re-mixes information token-by-token. | `block.mlp` |\n",
        "\n",
        "Below, we load the smallest public GPT-2 (124 M parameters), grab its *first* block, and inspect the pieces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47c87f6e",
      "metadata": {
        "id": "47c87f6e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# Load the 124 M-parameter GPT-2 and inspect its layers (12 layers)\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92df06df",
      "metadata": {
        "id": "92df06df"
      },
      "outputs": [],
      "source": [
        "# Run a tiny forward pass through the first block\n",
        "seq_len = 8\n",
        "dummy_tokens = torch.randint(0, gpt2.config.vocab_size, (1, seq_len))\n",
        "with torch.no_grad():\n",
        "    # Embed tokens + positions the same way GPT-2 does\n",
        "    # Forward through one layer\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE\n",
        "    \"\"\"\n",
        "\n",
        "print(\"\\nOutput shape :\", out.shape) # (batch, seq_len, hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8493ecc8",
      "metadata": {
        "id": "8493ecc8"
      },
      "source": [
        "### 2.3 - Inside GPT-2\n",
        "\n",
        "GPT-2 is just many of those modules arranged in a repeating *block*. Let's print the modules inside the Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78ddee1",
      "metadata": {
        "id": "a78ddee1"
      },
      "outputs": [],
      "source": [
        "# Print the name and modules inside gpt2\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed029847",
      "metadata": {
        "id": "ed029847"
      },
      "source": [
        "As you can see, the Transformer holds various modules, arranged from a list of blocks (`h`). The following table summarizes these modules:\n",
        "\n",
        "| Step | What it does | Why it matters |\n",
        "|------|--------------|----------------|\n",
        "| **Token ‚Üí Embedding** | Converts IDs to vectors | Gives the model a numeric ‚Äúhandle‚Äù on words |\n",
        "| **Positional Encoding** | Adds ‚Äúwhere am I?‚Äù info | Order matters in language |\n",
        "| **Multi-Head Self-Attention** | Each token asks ‚Äúwhich other tokens should I look at?‚Äù | Lets the model relate words across a sentence |\n",
        "| **Feed-Forward Network** | Two stacked Linear layers with a non-linearity | Mixes information and adds depth |\n",
        "| **LayerNorm & Residual** | Stabilize training and help gradients flow | Keeps very deep networks trainable |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6a7495",
      "metadata": {
        "id": "0a6a7495"
      },
      "source": [
        "### 2.4 LLM's output\n",
        "\n",
        "Passing a token sequence through an **LLM** yields a tensor of **logits** with shape  \n",
        "`(batch_size, seq_len, vocab_size)`.  \n",
        "Applying `softmax` on the last dimension turns those logits into probabilities.\n",
        "\n",
        "The cell below feeds an 8-token dummy sequence, prints the logits shape, and shows the five most likely next tokens for the final position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98b7b34",
      "metadata": {
        "id": "f98b7b34"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "\n",
        "# Load gpt2 model and tokenizer\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize input text\n",
        "text = \"Hello my name\"\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\"\n",
        "\n",
        "# Get logits by passing the ids to the gpt2 model.\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\"\n",
        "\n",
        "print(\"Logits shape :\", logits.shape)\n",
        "\n",
        "# Predict next token\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nTop-5 predictions for the next token:\")\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eb05c9b",
      "metadata": {
        "id": "0eb05c9b"
      },
      "source": [
        "### 2.5 - Key Takeaway\n",
        "\n",
        "A language model is nothing mystical: it‚Äôs a *huge composition* of small, understandable layers trained to predict the next token in a sequence of tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ccf391",
      "metadata": {
        "id": "e0ccf391"
      },
      "source": [
        "# 3 - Generation\n",
        "Once an LLM is trained to predict the probabilities, we can generate text from the model. This process is called decoding or sampling.\n",
        "\n",
        "At each step, the LLM outputs a **probability distribution** over the next token. It is the job of the decoding algorithm to pick the next token, and move on to the next token. There are different decoding algorithms and hyper-parameters to control the generaiton:\n",
        "* **Greedy** ‚Üí pick the single highest‚Äëprobability token each step (safe but repetitive).  \n",
        "* **Top‚Äëk / Nucleus (top‚Äëp)** ‚Üí sample from a subset of likely tokens (adds variety).\n",
        "* **beam** -> applies beam search to pick tokens\n",
        "* **Temperature** ‚Üí a *creativity* knob. Higher values flatten the probability distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac0c5728",
      "metadata": {
        "id": "ac0c5728"
      },
      "source": [
        "### 3.1 - Greedy decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2cb953",
      "metadata": {
        "id": "2f2cb953"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "MODELS = {\n",
        "    \"gpt2\": \"gpt2\",\n",
        "}\n",
        "tokenizers, models = {}, {}\n",
        "# Load models and tokenizers\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\"\n",
        "\n",
        "def generate(model_key, prompt, strategy=\"greedy\", max_new_tokens=100):\n",
        "    tok, mdl = tokenizers[model_key], models[model_key]\n",
        "    # Return the generations based on the provided strategy: greedy, top_k, top_p\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe777ba",
      "metadata": {
        "id": "dbe777ba"
      },
      "outputs": [],
      "source": [
        "tests=[\"Once upon a time\",\"What is 2+2?\", \"Suggest a party theme.\"]\n",
        "for prompt in tests:\n",
        "    print(f\"\\n== GPT-2 | Greedy ==\")\n",
        "    print(generate(\"gpt2\", prompt, \"greedy\", 80))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f51d44b2",
      "metadata": {
        "id": "f51d44b2"
      },
      "source": [
        "\n",
        "Naively picking the single best token every time has the following issues in practice:\n",
        "\n",
        "* **Loop**: ‚ÄúThe cat is is is‚Ä¶‚Äù  \n",
        "* **Miss long-term payoff**: the highest-probability word *now* might paint you into a boring corner later."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91607661",
      "metadata": {
        "id": "91607661"
      },
      "source": [
        "### 3.2 - Top-k or top-p sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0633d4a3",
      "metadata": {
        "id": "0633d4a3"
      },
      "outputs": [],
      "source": [
        "\n",
        "tests=[\"Once upon a time\",\"What is 2+2?\", \"Suggest a party theme.\"]\n",
        "for prompt in tests:\n",
        "    print(f\"\\n== GPT-2 | Top-p ==\")\n",
        "    print(generate(\"gpt2\", prompt, \"top-p\", 40))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "004b4039",
      "metadata": {
        "id": "004b4039"
      },
      "source": [
        "### 3.3 - Try It Yourself\n",
        "\n",
        "1. Scroll to the list called `tests`.\n",
        "2. Swap in your own prompts or tweak the decoding strategy.  \n",
        "3. Re‚Äërun the cell and compare the vibes.\n",
        "\n",
        "> **Tip:** Try the same prompt with `greedy` vs. `top_p` (0.9) and see how the tone changes. Notice especially how small temperature tweaks can soften or sharpen the prose.\n",
        "\n",
        "* `strategy`: `\"greedy\"`, `\"beam\"`, `\"top_k\"`, `\"top_p\"`  \n",
        "* `temperature`: `0.2 ‚Äì 2.0`  \n",
        "* `k` or `p` thresholds\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b775b02",
      "metadata": {
        "id": "6b775b02"
      },
      "source": [
        "# 4 - Completion vs. Instruction-tuned LLMs\n",
        "\n",
        "We have seen that we can use GPT2 model to pass an input text and generate a new text. However, this model only continues the provided text. It is not engaging in a dialouge-like conversation and cannot be helpful by answering instructions. On the other hand, **instruction-tuned LLMs** like `Qwen-Chat` go through an extra training stage called **post-training** after the base ‚Äúcompletion‚Äù model is finished. Because of post-training step, an instruction-tuned LLM will:\n",
        "\n",
        "* **Read the entire prompt as a request,** not just as text to mimic.  \n",
        "* **Stay in dialogue mode**. Answer questions, follow steps, ask clarifying queries.  \n",
        "* **Refuse or safe-complete** when instructions are unsafe or disallowed.  \n",
        "* **Adopt a consistent persona** (e.g., ‚ÄúAssistant‚Äù) rather than drifting into story continuation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1706dc08",
      "metadata": {
        "id": "1706dc08"
      },
      "source": [
        "### 4.1 - Qwen1.5-8B vs. GPT2\n",
        "\n",
        "In the code below we‚Äôll feed the same prompt to:\n",
        "\n",
        "* **GPT-2 (completion-only)** ‚Äì it will simply keep writing in the same style.  \n",
        "* **Qwen-Chat (instruction-tuned)** ‚Äì it will obey the instruction and respond directly.\n",
        "\n",
        "Comparing the two outputs makes the difference easy to see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b73e7a",
      "metadata": {
        "id": "57b73e7a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "MODELS = {\n",
        "    \"gpt2\": \"gpt2\",\n",
        "    \"qwen\": \"Qwen/Qwen1.5-1.8B-Chat\"\n",
        "}\n",
        "tokenizers, models = {}, {}\n",
        "# Load models and tokenizers\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef49ab1b",
      "metadata": {
        "id": "ef49ab1b"
      },
      "source": [
        "\n",
        "We downloaded two tiny checkpoints: `GPT‚Äë2` (124‚ÄØM parameters) and `Qwen‚Äë1.5‚ÄëChat` (1.8‚ÄØB). If the cell took a while, that was mostly network time. Models are stored locally after the first run.\n",
        "\n",
        "Let's now generate text and compare two models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c78a508",
      "metadata": {
        "id": "0c78a508"
      },
      "outputs": [],
      "source": [
        "\n",
        "tests=[(\"Once upon a time\",\"greedy\"),(\"What is 2+2?\",\"top_k\"),(\"Suggest a party theme.\",\"top_p\")]\n",
        "for prompt,strategy in tests:\n",
        "    for key in [\"gpt2\",\"qwen\"]:\n",
        "        print(f\"\\n== {key.upper()} | {strategy} ==\")\n",
        "        print(generate(key,prompt,strategy,80))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e1c3da1",
      "metadata": {
        "id": "8e1c3da1"
      },
      "source": [
        "# 5. (Optional) A Small LLM Playground"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "313ba974",
      "metadata": {
        "id": "313ba974"
      },
      "source": [
        "### 5.1‚ÄØ‚Äë Interactive Playground\n",
        "\n",
        "Enter a prompt, pick a model and decoding strategy, adjust the temperature, and press **Generate** to watch the model respond.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a67a884",
      "metadata": {
        "id": "1a67a884"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Make sure models and tokenizers are loaded\n",
        "try:\n",
        "    tokenizers\n",
        "    models\n",
        "except NameError:\n",
        "    raise RuntimeError(\"Please run the earlier setup cells that load the models before using the playground.\")\n",
        "\n",
        "def generate_playground(model_key, prompt, strategy=\"greedy\", temperature=1.0, max_new_tokens=100):\n",
        "    # Generation code\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE\n",
        "    \"\"\"\n",
        "\n",
        "# Your code to build boxes, dropdowns, and other elements in the UI using widgets and creating the UI using widgets.vbox and display.\n",
        "# Refer to https://ipywidgets.readthedocs.io/en/stable/\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbccead",
      "metadata": {
        "id": "cfbccead"
      },
      "source": [
        "\n",
        "## üéâ Congratulations!\n",
        "\n",
        "You‚Äôve just learned, explored, and inspected a real **LLM**. In one project you:\n",
        "* Learned how **tokenization** works in practice\n",
        "* Used `tiktoken` library to load and experiment with most advanced tokenizers.\n",
        "* Explored LLM architecture and inspected GPT2 blocks and layers\n",
        "* Learned decoding strategies and used `top-p` to generate text from GPT2\n",
        "* Loaded a powerful chat model, `Qwen1.5-8B` and generated text\n",
        "* Built an LLM playground\n",
        "\n",
        "\n",
        "üëè **Great job!** Take a moment to celebrate. You now have a working mental model of how LLMs work. The skills you used here power most LLMs you see everywhere.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30824bd6",
      "metadata": {
        "id": "30824bd6"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm_playground",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}